{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제1\n",
    "### Sklearn을 이용해서 iris 데이터를 읽어서 pandas dataframe에 저장해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import platform\n",
    "from matplotlib import font_manager, rc\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# matplotlib 한글대응\n",
    "get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n",
    "\n",
    "path = \"C:/Windows/Fonts/malgun.ttf\"\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    rc(\"font\", family=\"Arial Unicode MS\")\n",
    "elif platform.system() == \"Windows\":\n",
    "    font_name = font_manager.FontProperties(fname=path).get_name()\n",
    "    rc(\"font\", family=font_name)\n",
    "else:\n",
    "    print(\"Unknown system sorry.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris dataset 불러오기\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input data\n",
    "iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature\n",
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target\n",
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "pd_iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   species  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add target\n",
    "pd_iris['species'] = iris.target\n",
    "pd_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제2\n",
    "### Plotly express를 이용해서 네 개의 특성을 한 그래프에 boxplot으로 그려주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "variable=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa"
         },
         "name": "",
         "notched": false,
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "type": "box",
         "x": [
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)"
         ],
         "x0": " ",
         "xaxis": "x",
         "y": [
          5.1,
          4.9,
          4.7,
          4.6,
          5,
          5.4,
          4.6,
          5,
          4.4,
          4.9,
          5.4,
          4.8,
          4.8,
          4.3,
          5.8,
          5.7,
          5.4,
          5.1,
          5.7,
          5.1,
          5.4,
          5.1,
          4.6,
          5.1,
          4.8,
          5,
          5,
          5.2,
          5.2,
          4.7,
          4.8,
          5.4,
          5.2,
          5.5,
          4.9,
          5,
          5.5,
          4.9,
          4.4,
          5.1,
          5,
          4.5,
          4.4,
          5,
          5.1,
          4.8,
          5.1,
          4.6,
          5.3,
          5,
          7,
          6.4,
          6.9,
          5.5,
          6.5,
          5.7,
          6.3,
          4.9,
          6.6,
          5.2,
          5,
          5.9,
          6,
          6.1,
          5.6,
          6.7,
          5.6,
          5.8,
          6.2,
          5.6,
          5.9,
          6.1,
          6.3,
          6.1,
          6.4,
          6.6,
          6.8,
          6.7,
          6,
          5.7,
          5.5,
          5.5,
          5.8,
          6,
          5.4,
          6,
          6.7,
          6.3,
          5.6,
          5.5,
          5.5,
          6.1,
          5.8,
          5,
          5.6,
          5.7,
          5.7,
          6.2,
          5.1,
          5.7,
          6.3,
          5.8,
          7.1,
          6.3,
          6.5,
          7.6,
          4.9,
          7.3,
          6.7,
          7.2,
          6.5,
          6.4,
          6.8,
          5.7,
          5.8,
          6.4,
          6.5,
          7.7,
          7.7,
          6,
          6.9,
          5.6,
          7.7,
          6.3,
          6.7,
          7.2,
          6.2,
          6.1,
          6.4,
          7.2,
          7.4,
          7.9,
          6.4,
          6.3,
          6.1,
          7.7,
          6.3,
          6.4,
          6,
          6.9,
          6.7,
          6.9,
          5.8,
          6.8,
          6.7,
          6.7,
          6.3,
          6.5,
          6.2,
          5.9,
          3.5,
          3,
          3.2,
          3.1,
          3.6,
          3.9,
          3.4,
          3.4,
          2.9,
          3.1,
          3.7,
          3.4,
          3,
          3,
          4,
          4.4,
          3.9,
          3.5,
          3.8,
          3.8,
          3.4,
          3.7,
          3.6,
          3.3,
          3.4,
          3,
          3.4,
          3.5,
          3.4,
          3.2,
          3.1,
          3.4,
          4.1,
          4.2,
          3.1,
          3.2,
          3.5,
          3.6,
          3,
          3.4,
          3.5,
          2.3,
          3.2,
          3.5,
          3.8,
          3,
          3.8,
          3.2,
          3.7,
          3.3,
          3.2,
          3.2,
          3.1,
          2.3,
          2.8,
          2.8,
          3.3,
          2.4,
          2.9,
          2.7,
          2,
          3,
          2.2,
          2.9,
          2.9,
          3.1,
          3,
          2.7,
          2.2,
          2.5,
          3.2,
          2.8,
          2.5,
          2.8,
          2.9,
          3,
          2.8,
          3,
          2.9,
          2.6,
          2.4,
          2.4,
          2.7,
          2.7,
          3,
          3.4,
          3.1,
          2.3,
          3,
          2.5,
          2.6,
          3,
          2.6,
          2.3,
          2.7,
          3,
          2.9,
          2.9,
          2.5,
          2.8,
          3.3,
          2.7,
          3,
          2.9,
          3,
          3,
          2.5,
          2.9,
          2.5,
          3.6,
          3.2,
          2.7,
          3,
          2.5,
          2.8,
          3.2,
          3,
          3.8,
          2.6,
          2.2,
          3.2,
          2.8,
          2.8,
          2.7,
          3.3,
          3.2,
          2.8,
          3,
          2.8,
          3,
          2.8,
          3.8,
          2.8,
          2.8,
          2.6,
          3,
          3.4,
          3.1,
          3,
          3.1,
          3.1,
          3.1,
          2.7,
          3.2,
          3.3,
          3,
          2.5,
          3,
          3.4,
          3,
          1.4,
          1.4,
          1.3,
          1.5,
          1.4,
          1.7,
          1.4,
          1.5,
          1.4,
          1.5,
          1.5,
          1.6,
          1.4,
          1.1,
          1.2,
          1.5,
          1.3,
          1.4,
          1.7,
          1.5,
          1.7,
          1.5,
          1,
          1.7,
          1.9,
          1.6,
          1.6,
          1.5,
          1.4,
          1.6,
          1.6,
          1.5,
          1.5,
          1.4,
          1.5,
          1.2,
          1.3,
          1.4,
          1.3,
          1.5,
          1.3,
          1.3,
          1.3,
          1.6,
          1.9,
          1.4,
          1.6,
          1.4,
          1.5,
          1.4,
          4.7,
          4.5,
          4.9,
          4,
          4.6,
          4.5,
          4.7,
          3.3,
          4.6,
          3.9,
          3.5,
          4.2,
          4,
          4.7,
          3.6,
          4.4,
          4.5,
          4.1,
          4.5,
          3.9,
          4.8,
          4,
          4.9,
          4.7,
          4.3,
          4.4,
          4.8,
          5,
          4.5,
          3.5,
          3.8,
          3.7,
          3.9,
          5.1,
          4.5,
          4.5,
          4.7,
          4.4,
          4.1,
          4,
          4.4,
          4.6,
          4,
          3.3,
          4.2,
          4.2,
          4.2,
          4.3,
          3,
          4.1,
          6,
          5.1,
          5.9,
          5.6,
          5.8,
          6.6,
          4.5,
          6.3,
          5.8,
          6.1,
          5.1,
          5.3,
          5.5,
          5,
          5.1,
          5.3,
          5.5,
          6.7,
          6.9,
          5,
          5.7,
          4.9,
          6.7,
          4.9,
          5.7,
          6,
          4.8,
          4.9,
          5.6,
          5.8,
          6.1,
          6.4,
          5.6,
          5.1,
          5.6,
          6.1,
          5.6,
          5.5,
          4.8,
          5.4,
          5.6,
          5.1,
          5.1,
          5.9,
          5.7,
          5.2,
          5,
          5.2,
          5.4,
          5.1,
          0.2,
          0.2,
          0.2,
          0.2,
          0.2,
          0.4,
          0.3,
          0.2,
          0.2,
          0.1,
          0.2,
          0.2,
          0.1,
          0.1,
          0.2,
          0.4,
          0.4,
          0.3,
          0.3,
          0.3,
          0.2,
          0.4,
          0.2,
          0.5,
          0.2,
          0.2,
          0.4,
          0.2,
          0.2,
          0.2,
          0.2,
          0.4,
          0.1,
          0.2,
          0.2,
          0.2,
          0.2,
          0.1,
          0.2,
          0.2,
          0.3,
          0.3,
          0.2,
          0.6,
          0.4,
          0.3,
          0.2,
          0.2,
          0.2,
          0.2,
          1.4,
          1.5,
          1.5,
          1.3,
          1.5,
          1.3,
          1.6,
          1,
          1.3,
          1.4,
          1,
          1.5,
          1,
          1.4,
          1.3,
          1.4,
          1.5,
          1,
          1.5,
          1.1,
          1.8,
          1.3,
          1.5,
          1.2,
          1.3,
          1.4,
          1.4,
          1.7,
          1.5,
          1,
          1.1,
          1,
          1.2,
          1.6,
          1.5,
          1.6,
          1.5,
          1.3,
          1.3,
          1.3,
          1.2,
          1.4,
          1.2,
          1,
          1.3,
          1.2,
          1.3,
          1.3,
          1.1,
          1.3,
          2.5,
          1.9,
          2.1,
          1.8,
          2.2,
          2.1,
          1.7,
          1.8,
          1.8,
          2.5,
          2,
          1.9,
          2.1,
          2,
          2.4,
          2.3,
          1.8,
          2.2,
          2.3,
          1.5,
          2.3,
          2,
          2,
          1.8,
          2.1,
          1.8,
          1.8,
          1.8,
          2.1,
          1.6,
          1.9,
          2,
          2.2,
          1.5,
          1.4,
          2.3,
          2.4,
          1.8,
          1.8,
          2.1,
          2.4,
          2.3,
          1.9,
          2.3,
          2.5,
          2.3,
          1.9,
          2,
          2.3,
          1.8
         ],
         "y0": " ",
         "yaxis": "y"
        }
       ],
       "layout": {
        "boxmode": "group",
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "variable"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.box(pd_iris[iris.feature_names])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 3\n",
    "### Standard Scaler를 적용한 데이터를 또 다른 pandas dataframe에 저장해 주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- iris의 각 특성별로 0에서 표준편차의 몇배수 만큼 떨어져 있는지 나타내 줌으로써\n",
    "  실제 특성값의 크기에 상관없이 동일한 조건으로 비교할수 있게 scaling 해보기(Standard scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.900681</td>\n",
       "      <td>1.019004</td>\n",
       "      <td>-1.340227</td>\n",
       "      <td>-1.315444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.143017</td>\n",
       "      <td>-0.131979</td>\n",
       "      <td>-1.340227</td>\n",
       "      <td>-1.315444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.385353</td>\n",
       "      <td>0.328414</td>\n",
       "      <td>-1.397064</td>\n",
       "      <td>-1.315444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.506521</td>\n",
       "      <td>0.098217</td>\n",
       "      <td>-1.283389</td>\n",
       "      <td>-1.315444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.021849</td>\n",
       "      <td>1.249201</td>\n",
       "      <td>-1.340227</td>\n",
       "      <td>-1.315444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0          -0.900681          1.019004          -1.340227         -1.315444   \n",
       "1          -1.143017         -0.131979          -1.340227         -1.315444   \n",
       "2          -1.385353          0.328414          -1.397064         -1.315444   \n",
       "3          -1.506521          0.098217          -1.283389         -1.315444   \n",
       "4          -1.021849          1.249201          -1.340227         -1.315444   \n",
       "\n",
       "   species  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "SS = StandardScaler()\n",
    "\n",
    "ss_iris = pd.DataFrame(SS.fit_transform(pd_iris[iris.feature_names]), columns= iris.feature_names)\n",
    "\n",
    "# add target value to new data frame\n",
    "ss_iris['species'] = iris.target\n",
    "\n",
    "ss_iris.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 5), (150, 5))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_iris.shape, ss_iris.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제4\n",
    "### Standard Scaler를 적용한 데이터의 네 개의 특성을 한 그래프에 boxplot을 그려주세요. 이때 plotly express를 사용해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "variable=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa"
         },
         "name": "",
         "notched": false,
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "type": "box",
         "x": [
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal length (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "sepal width (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal length (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)",
          "petal width (cm)"
         ],
         "x0": " ",
         "xaxis": "x",
         "y": [
          -0.9006811702978099,
          -1.1430169111851116,
          -1.3853526520724144,
          -1.5065205225160663,
          -1.0218490407414607,
          -0.5371775589668552,
          -1.5065205225160663,
          -1.0218490407414607,
          -1.748856263403368,
          -1.1430169111851116,
          -0.5371775589668552,
          -1.2641847816287635,
          -1.2641847816287635,
          -1.87002413384702,
          -0.052506077192250644,
          -0.1736739476359015,
          -0.5371775589668552,
          -0.9006811702978099,
          -0.1736739476359015,
          -0.9006811702978099,
          -0.5371775589668552,
          -0.9006811702978099,
          -1.5065205225160663,
          -0.9006811702978099,
          -1.2641847816287635,
          -1.0218490407414607,
          -1.0218490407414607,
          -0.779513299854158,
          -0.779513299854158,
          -1.3853526520724144,
          -1.2641847816287635,
          -0.5371775589668552,
          -0.779513299854158,
          -0.4160096885232043,
          -1.1430169111851116,
          -1.0218490407414607,
          -0.4160096885232043,
          -1.1430169111851116,
          -1.748856263403368,
          -0.9006811702978099,
          -1.0218490407414607,
          -1.6276883929597172,
          -1.748856263403368,
          -1.0218490407414607,
          -0.9006811702978099,
          -1.2641847816287635,
          -0.9006811702978099,
          -1.5065205225160663,
          -0.6583454294105071,
          -1.0218490407414607,
          1.401508368131565,
          0.6745011454696578,
          1.2803404976879142,
          -0.4160096885232043,
          0.7956690159133086,
          -0.1736739476359015,
          0.5533332750260058,
          -1.1430169111851116,
          0.9168368863569595,
          -0.779513299854158,
          -1.0218490407414607,
          0.06866179325140129,
          0.18982966369505214,
          0.310997534138703,
          -0.29484181807955345,
          1.0380047568006114,
          -0.29484181807955345,
          -0.052506077192250644,
          0.4321654045823549,
          -0.29484181807955345,
          0.06866179325140129,
          0.310997534138703,
          0.5533332750260058,
          0.310997534138703,
          0.6745011454696578,
          0.9168368863569595,
          1.1591726272442622,
          1.0380047568006114,
          0.18982966369505214,
          -0.1736739476359015,
          -0.4160096885232043,
          -0.4160096885232043,
          -0.052506077192250644,
          0.18982966369505214,
          -0.5371775589668552,
          0.18982966369505214,
          1.0380047568006114,
          0.5533332750260058,
          -0.29484181807955345,
          -0.4160096885232043,
          -0.4160096885232043,
          0.310997534138703,
          -0.052506077192250644,
          -1.0218490407414607,
          -0.29484181807955345,
          -0.1736739476359015,
          -0.1736739476359015,
          0.4321654045823549,
          -0.9006811702978099,
          -0.1736739476359015,
          0.5533332750260058,
          -0.052506077192250644,
          1.522676238575216,
          0.5533332750260058,
          0.7956690159133086,
          2.1285155907934725,
          -1.1430169111851116,
          1.7650119794625188,
          1.0380047568006114,
          1.6438441090188678,
          0.7956690159133086,
          0.6745011454696578,
          1.1591726272442622,
          -0.1736739476359015,
          -0.052506077192250644,
          0.6745011454696578,
          0.7956690159133086,
          2.249683461237124,
          2.249683461237124,
          0.18982966369505214,
          1.2803404976879142,
          -0.29484181807955345,
          2.249683461237124,
          0.5533332750260058,
          1.0380047568006114,
          1.6438441090188678,
          0.4321654045823549,
          0.310997534138703,
          0.6745011454696578,
          1.6438441090188678,
          1.8861798499061706,
          2.492019202124427,
          0.6745011454696578,
          0.5533332750260058,
          0.310997534138703,
          2.249683461237124,
          0.5533332750260058,
          0.6745011454696578,
          0.18982966369505214,
          1.2803404976879142,
          1.0380047568006114,
          1.2803404976879142,
          -0.052506077192250644,
          1.1591726272442622,
          1.0380047568006114,
          1.0380047568006114,
          0.5533332750260058,
          0.7956690159133086,
          0.4321654045823549,
          0.06866179325140129,
          1.0190043519716065,
          -0.1319794793216258,
          0.3284140531956675,
          0.09821728693702086,
          1.2492011182302531,
          1.939791417006192,
          0.7888075857129598,
          0.7888075857129598,
          -0.36217624558027245,
          0.09821728693702086,
          1.4793978844888998,
          0.7888075857129598,
          -0.1319794793216258,
          -0.1319794793216258,
          2.169988183264839,
          3.0907752482994253,
          1.939791417006192,
          1.0190043519716065,
          1.7095946507475455,
          1.7095946507475455,
          0.7888075857129598,
          1.4793978844888998,
          1.2492011182302531,
          0.5586108194543131,
          0.7888075857129598,
          -0.1319794793216258,
          0.7888075857129598,
          1.0190043519716065,
          0.7888075857129598,
          0.3284140531956675,
          0.09821728693702086,
          0.7888075857129598,
          2.400184949523484,
          2.630381715782132,
          0.09821728693702086,
          0.3284140531956675,
          1.0190043519716065,
          1.2492011182302531,
          -0.1319794793216258,
          0.7888075857129598,
          1.0190043519716065,
          -1.7433568431321513,
          0.3284140531956675,
          1.0190043519716065,
          1.7095946507475455,
          -0.1319794793216258,
          1.7095946507475455,
          0.3284140531956675,
          1.4793978844888998,
          0.5586108194543131,
          0.3284140531956675,
          0.3284140531956675,
          0.09821728693702086,
          -1.7433568431321513,
          -0.5923730118389191,
          -0.5923730118389191,
          0.5586108194543131,
          -1.5131600768735047,
          -0.36217624558027245,
          -0.8225697780975647,
          -2.43394714190809,
          -0.1319794793216258,
          -1.973553609390797,
          -0.36217624558027245,
          -0.36217624558027245,
          0.09821728693702086,
          -0.1319794793216258,
          -0.8225697780975647,
          -1.973553609390797,
          -1.282963310614858,
          0.3284140531956675,
          -0.5923730118389191,
          -1.282963310614858,
          -0.5923730118389191,
          -0.36217624558027245,
          -0.1319794793216258,
          -0.5923730118389191,
          -0.1319794793216258,
          -0.36217624558027245,
          -1.0527665443562113,
          -1.5131600768735047,
          -1.5131600768735047,
          -0.8225697780975647,
          -0.8225697780975647,
          -0.1319794793216258,
          0.7888075857129598,
          0.09821728693702086,
          -1.7433568431321513,
          -0.1319794793216258,
          -1.282963310614858,
          -1.0527665443562113,
          -0.1319794793216258,
          -1.0527665443562113,
          -1.7433568431321513,
          -0.8225697780975647,
          -0.1319794793216258,
          -0.36217624558027245,
          -0.36217624558027245,
          -1.282963310614858,
          -0.5923730118389191,
          0.5586108194543131,
          -0.8225697780975647,
          -0.1319794793216258,
          -0.36217624558027245,
          -0.1319794793216258,
          -0.1319794793216258,
          -1.282963310614858,
          -0.36217624558027245,
          -1.282963310614858,
          1.2492011182302531,
          0.3284140531956675,
          -0.8225697780975647,
          -0.1319794793216258,
          -1.282963310614858,
          -0.5923730118389191,
          0.3284140531956675,
          -0.1319794793216258,
          1.7095946507475455,
          -1.0527665443562113,
          -1.973553609390797,
          0.3284140531956675,
          -0.5923730118389191,
          -0.5923730118389191,
          -0.8225697780975647,
          0.5586108194543131,
          0.3284140531956675,
          -0.5923730118389191,
          -0.1319794793216258,
          -0.5923730118389191,
          -0.1319794793216258,
          -0.5923730118389191,
          1.7095946507475455,
          -0.5923730118389191,
          -0.5923730118389191,
          -1.0527665443562113,
          -0.1319794793216258,
          0.7888075857129598,
          0.09821728693702086,
          -0.1319794793216258,
          0.09821728693702086,
          0.09821728693702086,
          0.09821728693702086,
          -0.8225697780975647,
          0.3284140531956675,
          0.5586108194543131,
          -0.1319794793216258,
          -1.282963310614858,
          -0.1319794793216258,
          0.7888075857129598,
          -0.1319794793216258,
          -1.3402265266227635,
          -1.3402265266227635,
          -1.3970639535363667,
          -1.2833890997091604,
          -1.3402265266227635,
          -1.169714245881954,
          -1.3402265266227635,
          -1.2833890997091604,
          -1.3402265266227635,
          -1.2833890997091604,
          -1.2833890997091604,
          -1.2265516727955572,
          -1.3402265266227635,
          -1.5107388073635728,
          -1.4539013804499696,
          -1.2833890997091604,
          -1.3970639535363667,
          -1.3402265266227635,
          -1.169714245881954,
          -1.2833890997091604,
          -1.169714245881954,
          -1.2833890997091604,
          -1.567576234277176,
          -1.169714245881954,
          -1.056039392054748,
          -1.2265516727955572,
          -1.2265516727955572,
          -1.2833890997091604,
          -1.3402265266227635,
          -1.2265516727955572,
          -1.2265516727955572,
          -1.2833890997091604,
          -1.2833890997091604,
          -1.3402265266227635,
          -1.2833890997091604,
          -1.4539013804499696,
          -1.3970639535363667,
          -1.3402265266227635,
          -1.3970639535363667,
          -1.2833890997091604,
          -1.3970639535363667,
          -1.3970639535363667,
          -1.3970639535363667,
          -1.2265516727955572,
          -1.056039392054748,
          -1.3402265266227635,
          -1.2265516727955572,
          -1.3402265266227635,
          -1.2833890997091604,
          -1.3402265266227635,
          0.5354085615261401,
          0.42173370769893376,
          0.6490834153533465,
          0.13754657313091806,
          0.4785711346125367,
          0.42173370769893376,
          0.5354085615261401,
          -0.26031541526430396,
          0.4785711346125367,
          0.08070914621731488,
          -0.1466405614370976,
          0.25122142695812444,
          0.13754657313091806,
          0.5354085615261401,
          -0.08980313452349442,
          0.3648962807853308,
          0.42173370769893376,
          0.194384000044521,
          0.42173370769893376,
          0.08070914621731488,
          0.5922459884397431,
          0.13754657313091806,
          0.6490834153533465,
          0.5354085615261401,
          0.3080588538717274,
          0.3648962807853308,
          0.5922459884397431,
          0.7059208422669494,
          0.42173370769893376,
          -0.1466405614370976,
          0.023871719303711698,
          -0.03296570760989124,
          0.08070914621731488,
          0.7627582691805523,
          0.42173370769893376,
          0.42173370769893376,
          0.5354085615261401,
          0.3648962807853308,
          0.194384000044521,
          0.13754657313091806,
          0.3648962807853308,
          0.4785711346125367,
          0.13754657313091806,
          -0.26031541526430396,
          0.25122142695812444,
          0.25122142695812444,
          0.25122142695812444,
          0.3080588538717274,
          -0.4308276960051133,
          0.194384000044521,
          1.2742951114029808,
          0.7627582691805523,
          1.2174576844893779,
          1.0469454037485681,
          1.1606202575757745,
          1.6153196728845993,
          0.42173370769893376,
          1.44480739214379,
          1.1606202575757745,
          1.3311325383165837,
          0.7627582691805523,
          0.8764331230077588,
          0.9901079768349651,
          0.7059208422669494,
          0.7627582691805523,
          0.8764331230077588,
          0.9901079768349651,
          1.672157099798203,
          1.7858319536254093,
          0.7059208422669494,
          1.1037828306621715,
          0.6490834153533465,
          1.672157099798203,
          0.6490834153533465,
          1.1037828306621715,
          1.2742951114029808,
          0.5922459884397431,
          0.6490834153533465,
          1.0469454037485681,
          1.1606202575757745,
          1.3311325383165837,
          1.5016448190573937,
          1.0469454037485681,
          0.7627582691805523,
          1.0469454037485681,
          1.3311325383165837,
          1.0469454037485681,
          0.9901079768349651,
          0.5922459884397431,
          0.9332705499213622,
          1.0469454037485681,
          0.7627582691805523,
          0.7627582691805523,
          1.2174576844893779,
          1.1037828306621715,
          0.8195956960941558,
          0.7059208422669494,
          0.8195956960941558,
          0.9332705499213622,
          0.7627582691805523,
          -1.3154442950077407,
          -1.3154442950077407,
          -1.3154442950077407,
          -1.3154442950077407,
          -1.3154442950077407,
          -1.052179926427139,
          -1.18381211071744,
          -1.3154442950077407,
          -1.3154442950077407,
          -1.4470764792980415,
          -1.3154442950077407,
          -1.3154442950077407,
          -1.4470764792980415,
          -1.4470764792980415,
          -1.3154442950077407,
          -1.052179926427139,
          -1.052179926427139,
          -1.18381211071744,
          -1.18381211071744,
          -1.18381211071744,
          -1.3154442950077407,
          -1.052179926427139,
          -1.3154442950077407,
          -0.9205477421368382,
          -1.3154442950077407,
          -1.3154442950077407,
          -1.052179926427139,
          -1.3154442950077407,
          -1.3154442950077407,
          -1.3154442950077407,
          -1.3154442950077407,
          -1.052179926427139,
          -1.4470764792980415,
          -1.3154442950077407,
          -1.3154442950077407,
          -1.3154442950077407,
          -1.3154442950077407,
          -1.4470764792980415,
          -1.3154442950077407,
          -1.3154442950077407,
          -1.18381211071744,
          -1.18381211071744,
          -1.3154442950077407,
          -0.7889155578465374,
          -1.052179926427139,
          -1.18381211071744,
          -1.3154442950077407,
          -1.3154442950077407,
          -1.3154442950077407,
          -1.3154442950077407,
          0.2641419164758693,
          0.3957741007661703,
          0.3957741007661703,
          0.13250973218556866,
          0.3957741007661703,
          0.13250973218556866,
          0.5274062850564712,
          -0.262386820685334,
          0.13250973218556866,
          0.2641419164758693,
          -0.262386820685334,
          0.3957741007661703,
          -0.262386820685334,
          0.2641419164758693,
          0.13250973218556866,
          0.2641419164758693,
          0.3957741007661703,
          -0.262386820685334,
          0.3957741007661703,
          -0.13075463639503299,
          0.7906706536370729,
          0.13250973218556866,
          0.3957741007661703,
          0.0008775478952676988,
          0.13250973218556866,
          0.2641419164758693,
          0.2641419164758693,
          0.659038469346772,
          0.3957741007661703,
          -0.262386820685334,
          -0.13075463639503299,
          -0.262386820685334,
          0.0008775478952676988,
          0.5274062850564712,
          0.3957741007661703,
          0.5274062850564712,
          0.3957741007661703,
          0.13250973218556866,
          0.13250973218556866,
          0.13250973218556866,
          0.0008775478952676988,
          0.2641419164758693,
          0.0008775478952676988,
          -0.262386820685334,
          0.13250973218556866,
          0.0008775478952676988,
          0.13250973218556866,
          0.13250973218556866,
          -0.13075463639503299,
          0.13250973218556866,
          1.712095943669179,
          0.9223028379273737,
          1.1855672065079756,
          0.7906706536370729,
          1.3171993907982766,
          1.1855672065079756,
          0.659038469346772,
          0.7906706536370729,
          0.7906706536370729,
          1.712095943669179,
          1.0539350222176747,
          0.9223028379273737,
          1.1855672065079756,
          1.0539350222176747,
          1.580463759378878,
          1.448831575088577,
          0.7906706536370729,
          1.3171993907982766,
          1.448831575088577,
          0.3957741007661703,
          1.448831575088577,
          1.0539350222176747,
          1.0539350222176747,
          0.7906706536370729,
          1.1855672065079756,
          0.7906706536370729,
          0.7906706536370729,
          0.7906706536370729,
          1.1855672065079756,
          0.5274062850564712,
          0.9223028379273737,
          1.0539350222176747,
          1.3171993907982766,
          0.3957741007661703,
          0.2641419164758693,
          1.448831575088577,
          1.580463759378878,
          0.7906706536370729,
          0.7906706536370729,
          1.1855672065079756,
          1.580463759378878,
          1.448831575088577,
          0.9223028379273737,
          1.448831575088577,
          1.712095943669179,
          1.448831575088577,
          0.9223028379273737,
          1.0539350222176747,
          1.448831575088577,
          0.7906706536370729
         ],
         "y0": " ",
         "yaxis": "y"
        }
       ],
       "layout": {
        "boxmode": "group",
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "variable"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.box(ss_iris[iris.feature_names])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 5\n",
    "### Standard Scaler를 적용한 데이터를 8:2로 train, test 데이터로 나눠주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = ss_iris.drop(['species'], axis=1)\n",
    "y = ss_iris['species']\n",
    "\n",
    "# train set와 test set에서 품종별 비율을 맞춰주기 위해 stratify option 적용\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 4), (120,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 4), (30,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 6\n",
    "### 5번의 train 데이터에 AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, DecisionTreeClassifier, LogisticRegression, LGBMClassifier 모델들을 이용해서 분류학습을 시켜주세요. 이때 각각의 모델의 test 데이터에 대한 accuracy를 제시해 주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirement\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass 분류이기 때문에 average 속성값을 적용시켜 줘야함\n",
    "def get_clf_result(y_test, pred, pred_prob):\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    pre = precision_score(y_test, pred, average='micro')\n",
    "    re = recall_score (y_test, pred, average='micro')\n",
    "    f1 = f1_score(y_test, pred, average='macro')\n",
    "    auc = roc_auc_score(y_test, pred_prob, multi_class=\"ovr\")\n",
    "\n",
    "    return acc, pre, re, f1, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('AdaBoostClassifier', AdaBoostClassifier()),\n",
    "    ('GradientBoostingClassifier', GradientBoostingClassifier()),\n",
    "    ('RandomForestClassifier', RandomForestClassifier()),\n",
    "    ('DecisionTreeClassifier', DecisionTreeClassifier()),\n",
    "    ('LogisticRegression', LogisticRegression()),\n",
    "    ('LGBMClassifier', LGBMClassifier())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: AdaBoostClassifier\n",
      "==== Confusion matrix =========\n",
      "[[10  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  0 10]]\n",
      "=============================\n",
      "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1:  1.0000, AUC:  1.0000\n",
      "=============================\n",
      "Model Name: GradientBoostingClassifier\n",
      "==== Confusion matrix =========\n",
      "[[10  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  0 10]]\n",
      "=============================\n",
      "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1:  1.0000, AUC:  1.0000\n",
      "=============================\n",
      "Model Name: RandomForestClassifier\n",
      "==== Confusion matrix =========\n",
      "[[10  0  0]\n",
      " [ 0  9  1]\n",
      " [ 0  0 10]]\n",
      "=============================\n",
      "Accuracy: 0.9667, Precision: 0.9667, Recall: 0.9667, F1:  0.9666, AUC:  1.0000\n",
      "=============================\n",
      "Model Name: DecisionTreeClassifier\n",
      "==== Confusion matrix =========\n",
      "[[10  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  0 10]]\n",
      "=============================\n",
      "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1:  1.0000, AUC:  1.0000\n",
      "=============================\n",
      "Model Name: LogisticRegression\n",
      "==== Confusion matrix =========\n",
      "[[10  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  0 10]]\n",
      "=============================\n",
      "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1:  1.0000, AUC:  1.0000\n",
      "=============================\n",
      "Model Name: LGBMClassifier\n",
      "==== Confusion matrix =========\n",
      "[[10  0  0]\n",
      " [ 0  9  1]\n",
      " [ 0  0 10]]\n",
      "=============================\n",
      "Accuracy: 0.9667, Precision: 0.9667, Recall: 0.9667, F1:  0.9666, AUC:  1.0000\n",
      "=============================\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "tmp = copy.deepcopy(models)\n",
    "\n",
    "for name, model in tmp:\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    pred_prob = model.predict_proba(X_test)\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    acc, pre, re, f1, auc = get_clf_result(y_test, pred, pred_prob)\n",
    "    print(f'Model Name: {name}')\n",
    "    print('==== Confusion matrix =========')\n",
    "    print(confusion)\n",
    "    print('=============================')\n",
    "    print(f'Accuracy: {acc:.4f}, Precision: {pre:.4f}, Recall: {re:.4f}, F1: {f1: .4f}, AUC: {auc: .4f}')\n",
    "    print('=============================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 7\n",
    "### 6번의 상황에서 train 데이터와 test 데이터의 accuracy를 모델별로 pandas dataframe에 정리해 주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names= ['Accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "model_names = [x[0] for x in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_info = []\n",
    "\n",
    "tmp = copy.deepcopy(models)\n",
    "\n",
    "for name, model in tmp:\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_train)\n",
    "    pred_prob = model.predict_proba(X_train)\n",
    "    train_info.append(get_clf_result(y_train, pred, pred_prob))\n",
    "\n",
    "df_train_result = pd.DataFrame(train_info, columns= col_names, index=model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.949969</td>\n",
       "      <td>0.995938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingClassifier</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.958327</td>\n",
       "      <td>0.998333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Accuracy  precision    recall        f1   roc_auc\n",
       "AdaBoostClassifier          0.950000   0.950000  0.950000  0.949969  0.995938\n",
       "GradientBoostingClassifier  1.000000   1.000000  1.000000  1.000000  1.000000\n",
       "RandomForestClassifier      1.000000   1.000000  1.000000  1.000000  1.000000\n",
       "DecisionTreeClassifier      1.000000   1.000000  1.000000  1.000000  1.000000\n",
       "LogisticRegression          0.958333   0.958333  0.958333  0.958327  0.998333\n",
       "LGBMClassifier              1.000000   1.000000  1.000000  1.000000  1.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_result # train dataset accuracy 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "test_info = []\n",
    "\n",
    "tmp = copy.deepcopy(models)\n",
    "\n",
    "for name, model in tmp:\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    pred_prob = model.predict_proba(X_test)\n",
    "    test_info.append(get_clf_result(y_test, pred, pred_prob))\n",
    "\n",
    "df_test_result = pd.DataFrame(test_info, columns= col_names, index=model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingClassifier</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966583</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Accuracy  precision    recall        f1  roc_auc\n",
       "AdaBoostClassifier          1.000000   1.000000  1.000000  1.000000      1.0\n",
       "GradientBoostingClassifier  1.000000   1.000000  1.000000  1.000000      1.0\n",
       "RandomForestClassifier      1.000000   1.000000  1.000000  1.000000      1.0\n",
       "DecisionTreeClassifier      1.000000   1.000000  1.000000  1.000000      1.0\n",
       "LogisticRegression          1.000000   1.000000  1.000000  1.000000      1.0\n",
       "LGBMClassifier              0.966667   0.966667  0.966667  0.966583      1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_result # test dataset accuracy 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제8\n",
    "### 6번의 상황에서 5겹 kFold해서 cross validation score를 계산해서, 각 모델별로 각 score의 평균과 표준편차를 제시해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirement import\n",
    "from sklearn.model_selection import KFold, cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AdaBoostClassifier, Mean: 0.9250, Std: 0.0486\n",
      "Model: GradientBoostingClassifier, Mean: 0.9500, Std: 0.0408\n",
      "Model: RandomForestClassifier, Mean: 0.9417, Std: 0.0565\n",
      "Model: DecisionTreeClassifier, Mean: 0.9417, Std: 0.0425\n",
      "Model: LogisticRegression, Mean: 0.9417, Std: 0.0425\n",
      "Model: LGBMClassifier, Mean: 0.9417, Std: 0.0425\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "clf_names = []\n",
    "\n",
    "tmp = copy.deepcopy(models)\n",
    "\n",
    "for name, model in tmp:\n",
    "    kfold = KFold(n_splits=5, random_state=13, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    clf_names.append(name)\n",
    "\n",
    "    print(f'Model: {name}, Mean: {cv_results.mean():.4f}, Std: {cv_results.std():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제9\n",
    "### 8번의 상황에서 각 모델별 cv score를 boxplot으로 그려서 비교해주세요. 여러분들은 어떤 모델이 가장 좋다고 생각하나요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(14, 8))\n",
    "fig.suptitle('Compare Classifier Score')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(clf_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 6개의 분류기가 거의 비슷한 성능을 보이고 있다. 박스플랏으로만 봣을때는 AdaBoostClassifier를 제외한 나머지 5개의 모델의 성능이 대체로 비슷함을 알수 있다.\n",
    "그 중 특히 분류기 cv_results의 평균과 표준편차를 살펴보게 되면 GradientBoostingClassifier가 평균 스코어가 제일 높고 표준편차가 제일 적으므로 6개 분류기중엔 가장 적합한 분류 모델이라고 할 수 있을것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제10\n",
    "### 9번의 상황에서 각 fold별 score를 모델별로 pandas dataframe에 정리해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_info = []\n",
    "\n",
    "tmp = copy.deepcopy(models)\n",
    "\n",
    "for name, model in tmp:\n",
    "    kfold = KFold(n_splits=5, random_state=13, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    cv_results_info.append(cv_results.tolist() + [cv_results.mean(), cv_results.std()])\n",
    "\n",
    "df_cv_result = pd.DataFrame(cv_results_info, columns=['Case1', 'Case2', 'Case3', 'Case4', 'Case5','Mean Score', 'Std Score'], index=model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case1</th>\n",
       "      <th>Case2</th>\n",
       "      <th>Case3</th>\n",
       "      <th>Case4</th>\n",
       "      <th>Case5</th>\n",
       "      <th>Mean Score</th>\n",
       "      <th>Std Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.048591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingClassifier</th>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.040825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.056519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.042492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.042492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.042492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Case1  Case2     Case3     Case4     Case5  \\\n",
       "AdaBoostClassifier          0.875000    1.0  0.875000  0.958333  0.916667   \n",
       "GradientBoostingClassifier  0.958333    1.0  0.875000  0.958333  0.958333   \n",
       "RandomForestClassifier      0.958333    1.0  0.833333  0.958333  0.958333   \n",
       "DecisionTreeClassifier      0.916667    1.0  0.875000  0.958333  0.958333   \n",
       "LogisticRegression          0.958333    1.0  0.875000  0.916667  0.958333   \n",
       "LGBMClassifier              0.958333    1.0  0.875000  0.958333  0.916667   \n",
       "\n",
       "                            Mean Score  Std Score  \n",
       "AdaBoostClassifier            0.925000   0.048591  \n",
       "GradientBoostingClassifier    0.950000   0.040825  \n",
       "RandomForestClassifier        0.941667   0.056519  \n",
       "DecisionTreeClassifier        0.941667   0.042492  \n",
       "LogisticRegression            0.941667   0.042492  \n",
       "LGBMClassifier                0.941667   0.042492  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cv_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 11\n",
    "### 다시 원본 데이터에서 데이터를 test와 train으로 나눈 후, Standard scaler와 Decision Tree를 연달아 사용하는 pipeline을 꾸며 주세요. 그리고, 해당 pipeline을 cross validation을 수행해서 score를 제시해 주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   species  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_iris.head() #원본 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split datastet\n",
    "\n",
    "X_pp = pd_iris.drop(['species'], axis=1)\n",
    "y_pp = pd_iris['species']\n",
    "\n",
    "pp_X_train, pp_X_test, pp_y_train, pp_y_test = train_test_split(X_pp, y_pp, test_size=0.2, random_state=13, stratify= y_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 4), (120,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_X_train.shape, pp_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 4), (30,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_X_test.shape, pp_y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scaler', StandardScaler()),\n",
       " ('clf', DecisionTreeClassifier(random_state=13))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# requirement import\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "estimators = [\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', DecisionTreeClassifier())\n",
    "]\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.set_params(clf__random_state=13)\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1:  0.8750\n",
      "Case 2:  0.9167\n",
      "Case 3:  0.8750\n",
      "Case 4:  0.9167\n",
      "Case 5:  1.0000\n",
      "Mean Score: 0.9167\n",
      "Std Score: 0.0456\n"
     ]
    }
   ],
   "source": [
    "skfold = StratifiedKFold(n_splits=5)\n",
    "scores = cross_val_score(pipe, pp_X_train, pp_y_train, cv=skfold)\n",
    "for i, score in enumerate(scores):\n",
    "    print(f'Case {i + 1}: {score: .4f}')\n",
    "print(f'Mean Score: {scores.mean():.4f}')\n",
    "print(f'Std Score: {scores.std():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제12\n",
    "### 11번의 상황에서 6번의 모델들을 모두 pipeline에 각각 적용해주세요. 그리고 해당 pipeline을 cross validation을 수행해서 score를 제시해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AdaBoostClassifier', AdaBoostClassifier()),\n",
       " ('GradientBoostingClassifier', GradientBoostingClassifier()),\n",
       " ('RandomForestClassifier', RandomForestClassifier()),\n",
       " ('DecisionTreeClassifier', DecisionTreeClassifier()),\n",
       " ('LogisticRegression', LogisticRegression()),\n",
       " ('LGBMClassifier', LGBMClassifier())]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======AdaBoostClassifier=======\n",
      "Case 1:  0.8750  Case 2:  0.9167  Case 3:  0.9583  Case 4:  0.8750  Case 5:  1.0000  \n",
      "Mean Score: 0.9250, Std Score: 0.0486\n",
      "======GradientBoostingClassifier=======\n",
      "Case 1:  0.8750  Case 2:  0.9583  Case 3:  1.0000  Case 4:  0.9167  Case 5:  1.0000  \n",
      "Mean Score: 0.9500, Std Score: 0.0486\n",
      "======RandomForestClassifier=======\n",
      "Case 1:  0.8750  Case 2:  0.9583  Case 3:  1.0000  Case 4:  0.9167  Case 5:  1.0000  \n",
      "Mean Score: 0.9500, Std Score: 0.0486\n",
      "======DecisionTreeClassifier=======\n",
      "Case 1:  0.8750  Case 2:  0.9167  Case 3:  0.8750  Case 4:  0.9167  Case 5:  1.0000  \n",
      "Mean Score: 0.9167, Std Score: 0.0456\n",
      "======LogisticRegression=======\n",
      "Case 1:  0.8750  Case 2:  0.9583  Case 3:  1.0000  Case 4:  0.9583  Case 5:  1.0000  \n",
      "Mean Score: 0.9583, Std Score: 0.0456\n",
      "======LGBMClassifier=======\n",
      "Case 1:  0.8750  Case 2:  0.9583  Case 3:  0.9583  Case 4:  0.9167  Case 5:  1.0000  \n",
      "Mean Score: 0.9417, Std Score: 0.0425\n"
     ]
    }
   ],
   "source": [
    "tmp = copy.deepcopy(models)\n",
    "\n",
    "for name, model in tmp:\n",
    "    estimators = [\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', model)\n",
    "    ]\n",
    "\n",
    "    pipe = Pipeline(estimators)\n",
    "    pipe.set_params(clf__random_state=13)\n",
    "\n",
    "    skfold = StratifiedKFold(n_splits=5)\n",
    "    scores = cross_val_score(pipe, pp_X_train, pp_y_train, cv=skfold)\n",
    "    print(f'======{name}=======')\n",
    "    for i, score in enumerate(scores):\n",
    "        print(f'Case {i + 1}: {score: .4f} ', end=' ')\n",
    "    print()\n",
    "    print(f'Mean Score: {scores.mean():.4f}, Std Score: {scores.std():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 교차 검증을 각 파이프라인 모델별로 수행했을시 LogisticRegression 모델의 평균 점수가 가장 높게 나오는것으로 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 13\n",
    "### Pipeline에 분류기를 DecisionTree와 RandomForest를 적용한 후 GridSearchCV를 통해 최적의 모델과 파라미터를 찾아주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('clf',\n",
       "                                        DecisionTreeClassifier(random_state=13))]),\n",
       "             param_grid=[{'clf__max_depth': [2, 4, 7, 10]}])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', DecisionTreeClassifier(random_state=13))\n",
    "]\n",
    "param_grid = [{'clf__max_depth': [2, 4, 7, 10]}]\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "GridSearch = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=5)\n",
    "GridSearch.fit(pp_X_train, pp_y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'mean_fit_time': array([0.00373392, 0.00335836, 0.00319982, 0.00259905]),\n",
      "    'mean_score_time': array([0.00159926, 0.0011127 , 0.00100045, 0.00139952]),\n",
      "    'mean_test_score': array([0.94166667, 0.91666667, 0.91666667, 0.91666667]),\n",
      "    'param_clf__max_depth': masked_array(data=[2, 4, 7, 10],\n",
      "             mask=[False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      "    'params': [   {'clf__max_depth': 2},\n",
      "                  {'clf__max_depth': 4},\n",
      "                  {'clf__max_depth': 7},\n",
      "                  {'clf__max_depth': 10}],\n",
      "    'rank_test_score': array([1, 2, 2, 2]),\n",
      "    'split0_test_score': array([0.875, 0.875, 0.875, 0.875]),\n",
      "    'split1_test_score': array([0.91666667, 0.91666667, 0.91666667, 0.91666667]),\n",
      "    'split2_test_score': array([0.95833333, 0.875     , 0.875     , 0.875     ]),\n",
      "    'split3_test_score': array([0.95833333, 0.91666667, 0.91666667, 0.91666667]),\n",
      "    'split4_test_score': array([1., 1., 1., 1.]),\n",
      "    'std_fit_time': array([0.00074216, 0.00040682, 0.00040114, 0.00049197]),\n",
      "    'std_score_time': array([4.91989419e-04, 4.73571538e-04, 8.31393994e-07, 4.89979845e-04]),\n",
      "    'std_test_score': array([0.04249183, 0.04564355, 0.04564355, 0.04564355])}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(GridSearch.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf__max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003734</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>4.919894e-04</td>\n",
       "      <td>2</td>\n",
       "      <td>{'clf__max_depth': 2}</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.042492</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003358</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>4.735715e-04</td>\n",
       "      <td>4</td>\n",
       "      <td>{'clf__max_depth': 4}</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.045644</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>8.313940e-07</td>\n",
       "      <td>7</td>\n",
       "      <td>{'clf__max_depth': 7}</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.045644</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002599</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>4.899798e-04</td>\n",
       "      <td>10</td>\n",
       "      <td>{'clf__max_depth': 10}</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.045644</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.003734      0.000742         0.001599    4.919894e-04   \n",
       "1       0.003358      0.000407         0.001113    4.735715e-04   \n",
       "2       0.003200      0.000401         0.001000    8.313940e-07   \n",
       "3       0.002599      0.000492         0.001400    4.899798e-04   \n",
       "\n",
       "  param_clf__max_depth                  params  split0_test_score  \\\n",
       "0                    2   {'clf__max_depth': 2}              0.875   \n",
       "1                    4   {'clf__max_depth': 4}              0.875   \n",
       "2                    7   {'clf__max_depth': 7}              0.875   \n",
       "3                   10  {'clf__max_depth': 10}              0.875   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.916667           0.958333           0.958333                1.0   \n",
       "1           0.916667           0.875000           0.916667                1.0   \n",
       "2           0.916667           0.875000           0.916667                1.0   \n",
       "3           0.916667           0.875000           0.916667                1.0   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.941667        0.042492                1  \n",
       "1         0.916667        0.045644                2  \n",
       "2         0.916667        0.045644                2  \n",
       "3         0.916667        0.045644                2  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_result_dt_clf = pd.DataFrame(GridSearch.cv_results_)\n",
    "cv_result_dt_clf.sort_values(by=\"mean_test_score\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('clf', DecisionTreeClassifier(max_depth=2, random_state=13))])\n",
      "Best Parameter: {'clf__max_depth': 2}\n",
      "Best Score: 0.9416666666666668\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Estimator: {GridSearch.best_estimator_}\")\n",
    "print(f\"Best Parameter: {GridSearch.best_params_}\")\n",
    "print(f\"Best Score: {GridSearch.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DecisionTreeClassifier의 경우 max_depth가 2인 모델이 가장 높은 점수를 기록함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('clf',\n",
       "                                        RandomForestClassifier(random_state=13))]),\n",
       "             param_grid=[{'clf__max_depth': [4, 6, 8, 10],\n",
       "                          'clf__min_samples_split': [2, 3, 5, 10],\n",
       "                          'clf__n_estimators': [50, 100, 200]}])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', RandomForestClassifier(random_state=13))\n",
    "]\n",
    "\n",
    "param_grid = [{\n",
    "    'clf__max_depth': [4, 6, 8, 10],\n",
    "    'clf__n_estimators': [50, 100, 200],\n",
    "    'clf__min_samples_split': [2, 3, 5, 10],\n",
    "    }]\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "GridSearch = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=5)\n",
    "GridSearch.fit(pp_X_train, pp_y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'mean_fit_time': array([0.04989958, 0.09079981, 0.17636385, 0.0460115 , 0.08910294,\n",
      "       0.20110226, 0.0484098 , 0.08744035, 0.17835674, 0.043887  ,\n",
      "       0.09492431, 0.17952533, 0.04975815, 0.0919023 , 0.1812479 ,\n",
      "       0.04837427, 0.09238405, 0.17109227, 0.04817772, 0.10779066,\n",
      "       0.16741285, 0.05145016, 0.08769917, 0.17105041, 0.046875  ,\n",
      "       0.09353309, 0.17078695, 0.03647752, 0.07702732, 0.16656637,\n",
      "       0.04062347, 0.09290609, 0.17649555, 0.04599376, 0.09366608,\n",
      "       0.18181019, 0.04292812, 0.08771958, 0.16905193, 0.04825363,\n",
      "       0.08720107, 0.176231  , 0.04891453, 0.09069581, 0.1773561 ,\n",
      "       0.04619274, 0.0894011 , 0.17563739]),\n",
      "    'mean_score_time': array([0.00519867, 0.0084702 , 0.01973739, 0.00458841, 0.0031251 ,\n",
      "       0.01573238, 0.00219383, 0.00841937, 0.01565886, 0.00828853,\n",
      "       0.00912313, 0.01513014, 0.00654411, 0.00793738, 0.01245103,\n",
      "       0.00100608, 0.00860424, 0.0175837 , 0.00199976, 0.00781298,\n",
      "       0.01452532, 0.00280104, 0.00632353, 0.01173339, 0.        ,\n",
      "       0.00832667, 0.01352901, 0.01001101, 0.01605062, 0.01564188,\n",
      "       0.00624995, 0.        , 0.01256166, 0.00520601, 0.00705609,\n",
      "       0.01336827, 0.00625005, 0.00985651, 0.01491594, 0.00299554,\n",
      "       0.00799656, 0.01459851, 0.00540004, 0.00839496, 0.01439004,\n",
      "       0.00440774, 0.00799861, 0.01480012]),\n",
      "    'mean_test_score': array([0.94166667, 0.95      , 0.95      , 0.94166667, 0.95      ,\n",
      "       0.95      , 0.94166667, 0.95      , 0.95      , 0.95      ,\n",
      "       0.95      , 0.95      , 0.94166667, 0.95      , 0.95      ,\n",
      "       0.94166667, 0.95      , 0.95      , 0.94166667, 0.95      ,\n",
      "       0.95      , 0.95      , 0.95      , 0.95      , 0.94166667,\n",
      "       0.95      , 0.95      , 0.94166667, 0.95      , 0.95      ,\n",
      "       0.94166667, 0.95      , 0.95      , 0.95      , 0.95      ,\n",
      "       0.95      , 0.94166667, 0.95      , 0.95      , 0.94166667,\n",
      "       0.95      , 0.95      , 0.94166667, 0.95      , 0.95      ,\n",
      "       0.95      , 0.95      , 0.95      ]),\n",
      "    'param_clf__max_depth': masked_array(data=[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6,\n",
      "                   6, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "                   10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      "    'param_clf__min_samples_split': masked_array(data=[2, 2, 2, 3, 3, 3, 5, 5, 5, 10, 10, 10, 2, 2, 2, 3, 3,\n",
      "                   3, 5, 5, 5, 10, 10, 10, 2, 2, 2, 3, 3, 3, 5, 5, 5, 10,\n",
      "                   10, 10, 2, 2, 2, 3, 3, 3, 5, 5, 5, 10, 10, 10],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      "    'param_clf__n_estimators': masked_array(data=[50, 100, 200, 50, 100, 200, 50, 100, 200, 50, 100, 200,\n",
      "                   50, 100, 200, 50, 100, 200, 50, 100, 200, 50, 100, 200,\n",
      "                   50, 100, 200, 50, 100, 200, 50, 100, 200, 50, 100, 200,\n",
      "                   50, 100, 200, 50, 100, 200, 50, 100, 200, 50, 100, 200],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      "    'params': [   {   'clf__max_depth': 4,\n",
      "                      'clf__min_samples_split': 2,\n",
      "                      'clf__n_estimators': 50},\n",
      "                  {   'clf__max_depth': 4,\n",
      "                      'clf__min_samples_split': 2,\n",
      "                      'clf__n_estimators': 100},\n",
      "                  {   'clf__max_depth': 4,\n",
      "                      'clf__min_samples_split': 2,\n",
      "                      'clf__n_estimators': 200},\n",
      "                  {   'clf__max_depth': 4,\n",
      "                      'clf__min_samples_split': 3,\n",
      "                      'clf__n_estimators': 50},\n",
      "                  {   'clf__max_depth': 4,\n",
      "                      'clf__min_samples_split': 3,\n",
      "                      'clf__n_estimators': 100},\n",
      "                  {   'clf__max_depth': 4,\n",
      "                      'clf__min_samples_split': 3,\n",
      "                      'clf__n_estimators': 200},\n",
      "                  {   'clf__max_depth': 4,\n",
      "                      'clf__min_samples_split': 5,\n",
      "                      'clf__n_estimators': 50},\n",
      "                  {   'clf__max_depth': 4,\n",
      "                      'clf__min_samples_split': 5,\n",
      "                      'clf__n_estimators': 100},\n",
      "                  {   'clf__max_depth': 4,\n",
      "                      'clf__min_samples_split': 5,\n",
      "                      'clf__n_estimators': 200},\n",
      "                  {   'clf__max_depth': 4,\n",
      "                      'clf__min_samples_split': 10,\n",
      "                      'clf__n_estimators': 50},\n",
      "                  {   'clf__max_depth': 4,\n",
      "                      'clf__min_samples_split': 10,\n",
      "                      'clf__n_estimators': 100},\n",
      "                  {   'clf__max_depth': 4,\n",
      "                      'clf__min_samples_split': 10,\n",
      "                      'clf__n_estimators': 200},\n",
      "                  {   'clf__max_depth': 6,\n",
      "                      'clf__min_samples_split': 2,\n",
      "                      'clf__n_estimators': 50},\n",
      "                  {   'clf__max_depth': 6,\n",
      "                      'clf__min_samples_split': 2,\n",
      "                      'clf__n_estimators': 100},\n",
      "                  {   'clf__max_depth': 6,\n",
      "                      'clf__min_samples_split': 2,\n",
      "                      'clf__n_estimators': 200},\n",
      "                  {   'clf__max_depth': 6,\n",
      "                      'clf__min_samples_split': 3,\n",
      "                      'clf__n_estimators': 50},\n",
      "                  {   'clf__max_depth': 6,\n",
      "                      'clf__min_samples_split': 3,\n",
      "                      'clf__n_estimators': 100},\n",
      "                  {   'clf__max_depth': 6,\n",
      "                      'clf__min_samples_split': 3,\n",
      "                      'clf__n_estimators': 200},\n",
      "                  {   'clf__max_depth': 6,\n",
      "                      'clf__min_samples_split': 5,\n",
      "                      'clf__n_estimators': 50},\n",
      "                  {   'clf__max_depth': 6,\n",
      "                      'clf__min_samples_split': 5,\n",
      "                      'clf__n_estimators': 100},\n",
      "                  {   'clf__max_depth': 6,\n",
      "                      'clf__min_samples_split': 5,\n",
      "                      'clf__n_estimators': 200},\n",
      "                  {   'clf__max_depth': 6,\n",
      "                      'clf__min_samples_split': 10,\n",
      "                      'clf__n_estimators': 50},\n",
      "                  {   'clf__max_depth': 6,\n",
      "                      'clf__min_samples_split': 10,\n",
      "                      'clf__n_estimators': 100},\n",
      "                  {   'clf__max_depth': 6,\n",
      "                      'clf__min_samples_split': 10,\n",
      "                      'clf__n_estimators': 200},\n",
      "                  {   'clf__max_depth': 8,\n",
      "                      'clf__min_samples_split': 2,\n",
      "                      'clf__n_estimators': 50},\n",
      "                  {   'clf__max_depth': 8,\n",
      "                      'clf__min_samples_split': 2,\n",
      "                      'clf__n_estimators': 100},\n",
      "                  {   'clf__max_depth': 8,\n",
      "                      'clf__min_samples_split': 2,\n",
      "                      'clf__n_estimators': 200},\n",
      "                  {   'clf__max_depth': 8,\n",
      "                      'clf__min_samples_split': 3,\n",
      "                      'clf__n_estimators': 50},\n",
      "                  {   'clf__max_depth': 8,\n",
      "                      'clf__min_samples_split': 3,\n",
      "                      'clf__n_estimators': 100},\n",
      "                  {   'clf__max_depth': 8,\n",
      "                      'clf__min_samples_split': 3,\n",
      "                      'clf__n_estimators': 200},\n",
      "                  {   'clf__max_depth': 8,\n",
      "                      'clf__min_samples_split': 5,\n",
      "                      'clf__n_estimators': 50},\n",
      "                  {   'clf__max_depth': 8,\n",
      "                      'clf__min_samples_split': 5,\n",
      "                      'clf__n_estimators': 100},\n",
      "                  {   'clf__max_depth': 8,\n",
      "                      'clf__min_samples_split': 5,\n",
      "                      'clf__n_estimators': 200},\n",
      "                  {   'clf__max_depth': 8,\n",
      "                      'clf__min_samples_split': 10,\n",
      "                      'clf__n_estimators': 50},\n",
      "                  {   'clf__max_depth': 8,\n",
      "                      'clf__min_samples_split': 10,\n",
      "                      'clf__n_estimators': 100},\n",
      "                  {   'clf__max_depth': 8,\n",
      "                      'clf__min_samples_split': 10,\n",
      "                      'clf__n_estimators': 200},\n",
      "                  {   'clf__max_depth': 10,\n",
      "                      'clf__min_samples_split': 2,\n",
      "                      'clf__n_estimators': 50},\n",
      "                  {   'clf__max_depth': 10,\n",
      "                      'clf__min_samples_split': 2,\n",
      "                      'clf__n_estimators': 100},\n",
      "                  {   'clf__max_depth': 10,\n",
      "                      'clf__min_samples_split': 2,\n",
      "                      'clf__n_estimators': 200},\n",
      "                  {   'clf__max_depth': 10,\n",
      "                      'clf__min_samples_split': 3,\n",
      "                      'clf__n_estimators': 50},\n",
      "                  {   'clf__max_depth': 10,\n",
      "                      'clf__min_samples_split': 3,\n",
      "                      'clf__n_estimators': 100},\n",
      "                  {   'clf__max_depth': 10,\n",
      "                      'clf__min_samples_split': 3,\n",
      "                      'clf__n_estimators': 200},\n",
      "                  {   'clf__max_depth': 10,\n",
      "                      'clf__min_samples_split': 5,\n",
      "                      'clf__n_estimators': 50},\n",
      "                  {   'clf__max_depth': 10,\n",
      "                      'clf__min_samples_split': 5,\n",
      "                      'clf__n_estimators': 100},\n",
      "                  {   'clf__max_depth': 10,\n",
      "                      'clf__min_samples_split': 5,\n",
      "                      'clf__n_estimators': 200},\n",
      "                  {   'clf__max_depth': 10,\n",
      "                      'clf__min_samples_split': 10,\n",
      "                      'clf__n_estimators': 50},\n",
      "                  {   'clf__max_depth': 10,\n",
      "                      'clf__min_samples_split': 10,\n",
      "                      'clf__n_estimators': 100},\n",
      "                  {   'clf__max_depth': 10,\n",
      "                      'clf__min_samples_split': 10,\n",
      "                      'clf__n_estimators': 200}],\n",
      "    'rank_test_score': array([37,  1,  1, 37,  1,  1, 37,  1,  1,  1,  1,  1, 37,  1,  1, 37,  1,\n",
      "        1, 37,  1,  1,  1,  1,  1, 37,  1,  1, 37,  1,  1, 37,  1,  1,  1,\n",
      "        1,  1, 37,  1,  1, 37,  1,  1, 37,  1,  1,  1,  1,  1]),\n",
      "    'split0_test_score': array([0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875,\n",
      "       0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875,\n",
      "       0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875,\n",
      "       0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875,\n",
      "       0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875,\n",
      "       0.875, 0.875, 0.875]),\n",
      "    'split1_test_score': array([0.95833333, 0.95833333, 0.95833333, 0.95833333, 0.95833333,\n",
      "       0.95833333, 0.95833333, 0.95833333, 0.95833333, 0.95833333,\n",
      "       0.95833333, 0.95833333, 0.95833333, 0.95833333, 0.95833333,\n",
      "       0.95833333, 0.95833333, 0.95833333, 0.95833333, 0.95833333,\n",
      "       0.95833333, 0.95833333, 0.95833333, 0.95833333, 0.95833333,\n",
      "       0.95833333, 0.95833333, 0.95833333, 0.95833333, 0.95833333,\n",
      "       0.95833333, 0.95833333, 0.95833333, 0.95833333, 0.95833333,\n",
      "       0.95833333, 0.95833333, 0.95833333, 0.95833333, 0.95833333,\n",
      "       0.95833333, 0.95833333, 0.95833333, 0.95833333, 0.95833333,\n",
      "       0.95833333, 0.95833333, 0.95833333]),\n",
      "    'split2_test_score': array([0.95833333, 1.        , 1.        , 0.95833333, 1.        ,\n",
      "       1.        , 0.95833333, 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.95833333, 1.        , 1.        ,\n",
      "       0.95833333, 1.        , 1.        , 0.95833333, 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 0.95833333,\n",
      "       1.        , 1.        , 0.95833333, 1.        , 1.        ,\n",
      "       0.95833333, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 0.95833333, 1.        , 1.        , 0.95833333,\n",
      "       1.        , 1.        , 0.95833333, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        ]),\n",
      "    'split3_test_score': array([0.91666667, 0.91666667, 0.91666667, 0.91666667, 0.91666667,\n",
      "       0.91666667, 0.91666667, 0.91666667, 0.91666667, 0.91666667,\n",
      "       0.91666667, 0.91666667, 0.91666667, 0.91666667, 0.91666667,\n",
      "       0.91666667, 0.91666667, 0.91666667, 0.91666667, 0.91666667,\n",
      "       0.91666667, 0.91666667, 0.91666667, 0.91666667, 0.91666667,\n",
      "       0.91666667, 0.91666667, 0.91666667, 0.91666667, 0.91666667,\n",
      "       0.91666667, 0.91666667, 0.91666667, 0.91666667, 0.91666667,\n",
      "       0.91666667, 0.91666667, 0.91666667, 0.91666667, 0.91666667,\n",
      "       0.91666667, 0.91666667, 0.91666667, 0.91666667, 0.91666667,\n",
      "       0.91666667, 0.91666667, 0.91666667]),\n",
      "    'split4_test_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
      "    'std_fit_time': array([3.28296260e-03, 1.05568644e-02, 1.51058416e-02, 8.88175205e-04,\n",
      "       6.67487533e-03, 1.15112300e-02, 6.33631933e-03, 7.69179530e-03,\n",
      "       1.51783415e-02, 5.64199672e-03, 5.22638368e-03, 1.61500297e-02,\n",
      "       3.09959970e-03, 9.84702018e-03, 1.26687554e-02, 7.67711794e-03,\n",
      "       3.57272123e-03, 9.28600924e-03, 1.66662771e-03, 1.02312380e-02,\n",
      "       6.62987205e-03, 4.60893401e-03, 6.05873957e-03, 7.66487293e-03,\n",
      "       2.68472448e-06, 8.93899945e-03, 8.01375219e-03, 6.60257833e-03,\n",
      "       2.19619404e-03, 5.70761331e-03, 7.65409152e-03, 2.06996017e-03,\n",
      "       9.22537568e-03, 8.95796131e-04, 6.38562442e-03, 1.43108201e-02,\n",
      "       6.54318793e-03, 1.05477770e-02, 6.92410728e-03, 4.70723021e-03,\n",
      "       4.01164589e-04, 4.16587608e-03, 6.54672753e-04, 2.78817506e-03,\n",
      "       1.79060474e-03, 3.87029355e-04, 1.02194712e-03, 7.25228257e-04]),\n",
      "    'std_score_time': array([3.99194371e-04, 4.41773482e-03, 4.46505248e-03, 4.94757154e-04,\n",
      "       6.25019073e-03, 3.88520953e-04, 2.70429917e-03, 6.62029563e-03,\n",
      "       9.68467068e-04, 6.33611828e-03, 9.98800160e-04, 4.60818390e-04,\n",
      "       5.50202903e-03, 4.99253945e-03, 6.29763224e-03, 2.01215744e-03,\n",
      "       4.92926809e-04, 3.27213859e-03, 2.44919541e-03, 4.79996865e-03,\n",
      "       2.20306426e-03, 2.31550050e-03, 5.86509981e-03, 4.89791427e-03,\n",
      "       0.00000000e+00, 5.01225099e-03, 6.99665406e-03, 8.25605449e-03,\n",
      "       8.52803846e-04, 3.43879936e-05, 7.65459705e-03, 0.00000000e+00,\n",
      "       6.28228708e-03, 3.96599130e-04, 3.61175456e-03, 2.76848081e-03,\n",
      "       7.65471385e-03, 5.78229141e-03, 7.98585936e-03, 2.44588639e-03,\n",
      "       2.03140465e-05, 4.77572770e-04, 4.90195849e-04, 4.81223247e-04,\n",
      "       4.73008314e-04, 5.01099991e-04, 1.98675791e-06, 7.47577237e-04]),\n",
      "    'std_test_score': array([0.04249183, 0.04859127, 0.04859127, 0.04249183, 0.04859127,\n",
      "       0.04859127, 0.04249183, 0.04859127, 0.04859127, 0.04859127,\n",
      "       0.04859127, 0.04859127, 0.04249183, 0.04859127, 0.04859127,\n",
      "       0.04249183, 0.04859127, 0.04859127, 0.04249183, 0.04859127,\n",
      "       0.04859127, 0.04859127, 0.04859127, 0.04859127, 0.04249183,\n",
      "       0.04859127, 0.04859127, 0.04249183, 0.04859127, 0.04859127,\n",
      "       0.04249183, 0.04859127, 0.04859127, 0.04859127, 0.04859127,\n",
      "       0.04859127, 0.04249183, 0.04859127, 0.04859127, 0.04249183,\n",
      "       0.04859127, 0.04859127, 0.04249183, 0.04859127, 0.04859127,\n",
      "       0.04859127, 0.04859127, 0.04859127])}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(GridSearch.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('clf', RandomForestClassifier(max_depth=4, random_state=13))])\n",
      "Best Parameter: {'clf__max_depth': 4, 'clf__min_samples_split': 2, 'clf__n_estimators': 100}\n",
      "Best Score: 0.95\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Estimator: {GridSearch.best_estimator_}\")\n",
    "print(f\"Best Parameter: {GridSearch.best_params_}\")\n",
    "print(f\"Best Score: {GridSearch.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf__max_depth</th>\n",
       "      <th>param_clf__min_samples_split</th>\n",
       "      <th>param_clf__n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.175637</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'clf__max_depth': 10, 'clf__min_samples_split...</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.048591</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.169052</td>\n",
       "      <td>0.006924</td>\n",
       "      <td>0.014916</td>\n",
       "      <td>0.007986</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'clf__max_depth': 10, 'clf__min_samples_split...</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.048591</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.051450</td>\n",
       "      <td>0.004609</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>{'clf__max_depth': 6, 'clf__min_samples_split'...</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.048591</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.087699</td>\n",
       "      <td>0.006059</td>\n",
       "      <td>0.006324</td>\n",
       "      <td>0.005865</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf__max_depth': 6, 'clf__min_samples_split'...</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.048591</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.171050</td>\n",
       "      <td>0.007665</td>\n",
       "      <td>0.011733</td>\n",
       "      <td>0.004898</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'clf__max_depth': 6, 'clf__min_samples_split'...</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.048591</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.090800</td>\n",
       "      <td>0.010557</td>\n",
       "      <td>0.008470</td>\n",
       "      <td>0.004418</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf__max_depth': 4, 'clf__min_samples_split'...</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.048591</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.093533</td>\n",
       "      <td>0.008939</td>\n",
       "      <td>0.008327</td>\n",
       "      <td>0.005012</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf__max_depth': 8, 'clf__min_samples_split'...</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.048591</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.170787</td>\n",
       "      <td>0.008014</td>\n",
       "      <td>0.013529</td>\n",
       "      <td>0.006997</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'clf__max_depth': 8, 'clf__min_samples_split'...</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.048591</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.176231</td>\n",
       "      <td>0.004166</td>\n",
       "      <td>0.014599</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>{'clf__max_depth': 10, 'clf__min_samples_split...</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.048591</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.077027</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.016051</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf__max_depth': 8, 'clf__min_samples_split'...</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.048591</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "47       0.175637      0.000725         0.014800        0.000748   \n",
       "38       0.169052      0.006924         0.014916        0.007986   \n",
       "21       0.051450      0.004609         0.002801        0.002316   \n",
       "22       0.087699      0.006059         0.006324        0.005865   \n",
       "23       0.171050      0.007665         0.011733        0.004898   \n",
       "1        0.090800      0.010557         0.008470        0.004418   \n",
       "25       0.093533      0.008939         0.008327        0.005012   \n",
       "26       0.170787      0.008014         0.013529        0.006997   \n",
       "41       0.176231      0.004166         0.014599        0.000478   \n",
       "28       0.077027      0.002196         0.016051        0.000853   \n",
       "\n",
       "   param_clf__max_depth param_clf__min_samples_split param_clf__n_estimators  \\\n",
       "47                   10                           10                     200   \n",
       "38                   10                            2                     200   \n",
       "21                    6                           10                      50   \n",
       "22                    6                           10                     100   \n",
       "23                    6                           10                     200   \n",
       "1                     4                            2                     100   \n",
       "25                    8                            2                     100   \n",
       "26                    8                            2                     200   \n",
       "41                   10                            3                     200   \n",
       "28                    8                            3                     100   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "47  {'clf__max_depth': 10, 'clf__min_samples_split...              0.875   \n",
       "38  {'clf__max_depth': 10, 'clf__min_samples_split...              0.875   \n",
       "21  {'clf__max_depth': 6, 'clf__min_samples_split'...              0.875   \n",
       "22  {'clf__max_depth': 6, 'clf__min_samples_split'...              0.875   \n",
       "23  {'clf__max_depth': 6, 'clf__min_samples_split'...              0.875   \n",
       "1   {'clf__max_depth': 4, 'clf__min_samples_split'...              0.875   \n",
       "25  {'clf__max_depth': 8, 'clf__min_samples_split'...              0.875   \n",
       "26  {'clf__max_depth': 8, 'clf__min_samples_split'...              0.875   \n",
       "41  {'clf__max_depth': 10, 'clf__min_samples_split...              0.875   \n",
       "28  {'clf__max_depth': 8, 'clf__min_samples_split'...              0.875   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "47           0.958333                1.0           0.916667   \n",
       "38           0.958333                1.0           0.916667   \n",
       "21           0.958333                1.0           0.916667   \n",
       "22           0.958333                1.0           0.916667   \n",
       "23           0.958333                1.0           0.916667   \n",
       "1            0.958333                1.0           0.916667   \n",
       "25           0.958333                1.0           0.916667   \n",
       "26           0.958333                1.0           0.916667   \n",
       "41           0.958333                1.0           0.916667   \n",
       "28           0.958333                1.0           0.916667   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "47                1.0             0.95        0.048591                1  \n",
       "38                1.0             0.95        0.048591                1  \n",
       "21                1.0             0.95        0.048591                1  \n",
       "22                1.0             0.95        0.048591                1  \n",
       "23                1.0             0.95        0.048591                1  \n",
       "1                 1.0             0.95        0.048591                1  \n",
       "25                1.0             0.95        0.048591                1  \n",
       "26                1.0             0.95        0.048591                1  \n",
       "41                1.0             0.95        0.048591                1  \n",
       "28                1.0             0.95        0.048591                1  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_result_rf_clf = pd.DataFrame(GridSearch.cv_results_)\n",
    "cv_result_rf_clf.sort_values(by=\"mean_test_score\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RandomForestClassifier의 경우 'clf__max_depth': 4, 'clf__min_samples_split': 2, 'clf__n_estimators': 100 인 경우가 가장 최적의 모델로 평가되었고 점수가 가장 높게 나왔다."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fff8a2ebda7d7c31fff9abc7af7b4b1a85e6e317ed9dea4607b41f7b3e93769e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('ds_study': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
